import json
import argparse
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import re
import random


def remove_templates(text):
    """移除模板标记，例如<|...|>"""
    return re.sub(r'<\|.*?\|>', '', text).strip()


def process_data_file(input_filename):
    output_data = []
    with open(input_filename, 'r', encoding='utf-8') as infile:
        for line in infile:
            line = line.strip()
            if not line:
                continue
            data = json.loads(line)

            # 提取 instruction
            prompt = data.get('prompt', '')
            instruction_match = re.search(
                r'<\|start_header_id\|>user<\|end_header_id\|>\n\n(.*?)<\|eot_id\|>',
                prompt,
                re.DOTALL
            )
            if instruction_match:
                instruction = remove_templates(instruction_match.group(1))
            else:
                instruction = ''

            # 处理 chosen 和 rejected
            chosen_text = data.get('chosen', '')
            rejected_text = data.get('rejected', '')

            # 根据 results 字段确定是否需要交换 chosen 和 rejected
            if data.get('results', 1) == 0:
                chosen_text, rejected_text = rejected_text, chosen_text

            # 创建新的数据条目
            output_entry = {
                'instruction': instruction,
                'chosen': chosen_text,
                'rejected': rejected_text
            }
            output_data.append(output_entry)
    return output_data


def generate_rationale(input_file, output_file, model_name, temperature, max_tokens):
    """
    生成理由，并构造指定格式的数据。

    Args:
        input_file (str): 输入数据文件路径。
        output_file (str): 输出文件路径。
        model_name (str): vllm 模型名称。
        temperature (float): 生成随机性温度。
        max_tokens (int): 最大生成的 token 数量。
    """

    # 加载处理后的数据
    processed_data = process_data_file(input_file)

    # 初始化 vllm 模型和 tokenizer
    llm = LLM(model=model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    sampling_params = SamplingParams(
        temperature=temperature, max_tokens=max_tokens, skip_special_tokens=False)

    # 构造 prompts 列表
    conversation_list = []
    updated_labels = []
    updated_responses = []

    for item in processed_data:
        instruction = item["instruction"]
        response1 = item["chosen"]
        response2 = item["rejected"]

        # 随机选择 chosen 和 rejected
        if random.random() < 0.5:
            chosen = "Output (a)"
            rejected = "Output (b)"
            updated_labels.append((chosen, rejected))
            updated_responses.append((response1, response2))
        else:
            chosen = "Output (b)"
            rejected = "Output (a)"
            response1, response2 = response2, response1  # 交换 responses
            updated_labels.append((chosen, rejected))
            updated_responses.append((response1, response2))

        # 处理原始模型的结果
        conversation = construct_prompt(
            instruction, response1, response2, chosen, rejected)
        conversation_list.append(conversation)

    prompt_token_ids = [tokenizer.apply_chat_template(
        conversation, add_generation_prompt=True) for conversation in conversation_list]

    # Batch generate responses
    outputs = llm.generate(
        prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)

    # Print the first conversation prompt and its tokenized form
    print(
        f"Sampled conversation:\n{tokenizer.decode(prompt_token_ids[0], skip_special_tokens=False)}", end='')
    # Print the first generated text and its tokenized form
    print(f"{outputs[0].outputs[0].text.strip()}")
    generated_texts = [output.outputs[0].text.strip() for output in outputs]

    # 构造新数据
    formatted_data = []
    for generated_text, item, (chosen, rejected), (response1, response2) in zip(generated_texts, processed_data, updated_labels, updated_responses):
        # 构造符合格式的数据
        formatted_data.append({
            "system": "You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.",
            "instruction": f"""After giving a brief explanation, select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better.

You should first provide a brief explanation of your evaluation, and then always end your response with either "Therefore, Output (a) is better." or "Therefore, Output (b) is better." verbatim.
Do NOT say both / neither are good.
Do NOT output any other words.

# Instruction:
{item["instruction"]}

# Output (a):
{response1}

# Output (b):
{response2}

# Decision (Give a brief explanation of your evaluation followed by either "Therefore, Output (a) is better." or "Therefore, Output (b) is better." verbatim. In your explanation, you should always use "Output (a)" or "Output (b)" to refer to the two outputs respectively.):""",
            "input": "",
            "output": generated_text
        })

    # 保存结果到文件
    with open(output_file, 'w') as f:
        json.dump(formatted_data, f, indent=4)

    print(f"生成的数据已保存到 {output_file}")
    print(f"最终保存的数据数量: {len(formatted_data)}")


def construct_prompt(instruction, response1, response2, chosen, rejected):
    system = """You are an evaluation expert. Your goal is to provide the rationale based on the given evaluation result."""

    user = f"""As an evaluation expert, given an instruction and its two possible outputs, compare the outputs. Below are the instruction and its candidate outputs:

# Instruction:
{instruction}

# Output (a):
{response1}

# Output (b):
{response2}

Given that {chosen} is better than {rejected}, please provide the rationale and end with "Therefore, {chosen} is better." (Do NOT output any other words.):"""
    return [{"role": "system", "content": system}, {"role": "user", "content": user}]


if __name__ == "__main__":
    # 使用 argparse 解析命令行参数
    parser = argparse.ArgumentParser(
        description="Use vllm to generate rationales and format the data.")
    parser.add_argument("--input", type=str, required=True,
                        help="Path to the processed input JSON file.")
    parser.add_argument("--output", type=str, required=True,
                        help="Path to the output file.")
    parser.add_argument("--model", type=str,
                        help="Name of the vllm model to use.")
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="Temperature for generation randomness.")
    parser.add_argument("--max_tokens", type=int, default=512,
                        help="Maximum number of tokens to generate.")

    # 解析命令行参数
    args = parser.parse_args()

    # 调用主函数
    generate_rationale(
        input_file=args.input,
        output_file=args.output,
        model_name=args.model,
        temperature=args.temperature,
        max_tokens=args.max_tokens
    )

